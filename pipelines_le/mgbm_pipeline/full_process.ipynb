{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9b1a57-9275-4251-8942-24739d27660b",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12c36398-451d-47f2-80f6-b144335ca947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Import your custom modules. Adjust the module paths as needed.\n",
    "from src.data.load_data import loadTrainingData\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.features.create_feature_vectors import extract_features_with_expanding_window\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828a24e3-604f-480b-b4dc-d4b64d27e42f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05489049-de4b-4388-a44c-b1fdd0ffa790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data from: ../../training_setA/*.psv with max_files=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PSV Files: 100%|██████████████████| 1000/1000 [00:00<00:00, 1158.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data from: ../../training_setB/*.psv with max_files=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PSV Files: 100%|██████████████████| 1000/1000 [00:00<00:00, 1122.93it/s]\n"
     ]
    }
   ],
   "source": [
    "directories = ['../../training_setA/', '../../training_setB/']\n",
    "max_files = 1000  # Adjust as needed\n",
    "\n",
    "patient_dict = {}\n",
    "\n",
    "for directory in directories:\n",
    "    pattern = os.path.join(directory, \"*.psv\")\n",
    "    print(f\"\\nLoading data from: {pattern} with max_files={max_files}\")\n",
    "    patient_data = loadTrainingData(\n",
    "        pattern,\n",
    "        max_files,\n",
    "        ignore_columns=['Age', 'Gender', 'Unit1', 'Unit2', 'HospAdmTime']\n",
    "    )\n",
    "    patient_dict.update(patient_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db37c0e8-118e-47f3-9e01-b1fedb150dfd",
   "metadata": {},
   "source": [
    "# Create Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c028cab8-4b58-46ee-a4b9-3c3a7e81c9d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77415, 36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.it/s]\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.6s0:00<01:16, 25.96it\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    1.8s0:01<00:48, 40.47it\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    3.9s00:03<00:44, 41.73i\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:    6.9s00:06<00:40, 42.25i\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   11.2s00:11<00:47, 33.00i\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:   16.7s00:16<00:35, 38.39i\n",
      "[Parallel(n_jobs=-1)]: Done 866 tasks      | elapsed:   21.9s00:21<00:26, 42.58i\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:   28.3s00:28<00:21, 40.51\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:   37.9s00:37<00:12, 44.24\n",
      "extracting features with expanding window:  76%|▊| 1528/2000 [00:39<00:11, 40.88"
     ]
    }
   ],
   "source": [
    "feature_df = extract_features_with_expanding_window(patient_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808951b3",
   "metadata": {},
   "source": [
    " # Save or Load Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature_data(feature_df, file_path=\"feature_data.pkl\"):\n",
    "    feature_df.to_pickle(file_path)\n",
    "    print(f\"Feature data saved to {file_path}\")\n",
    "\n",
    "def load_feature_data(file_path=\"feature_data.pkl\"):\n",
    "    feature_df = pd.read_pickle(file_path)\n",
    "    print(f\"Feature data loaded from {file_path}\")\n",
    "    return feature_df\n",
    "\n",
    "save_feature_data(feature_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75734e",
   "metadata": {},
   "source": [
    "# Add/Remove features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4507f1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "457e773c",
   "metadata": {},
   "source": [
    "# Split Sets into Test and Train on Patient ID of Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group rows by patient_id to create a patient-wise dictionary.\n",
    "patient_groups = {patient_id: group \n",
    "                  for patient_id, group in feature_df.groupby(\"patient_id\")}\n",
    "\n",
    "# Create a new dictionary with keys indicating sepsis status.\n",
    "# For each patient, if any row's SepsisLabel equals 1, mark that patient as sepsis.\n",
    "labeled_patients = {}\n",
    "for patient_id, df in patient_groups.items():\n",
    "    # Check if the patient ever had sepsis\n",
    "    sepsis_label = \"1\" if df[\"SepsisLabel\"].any() else \"0\"\n",
    "    new_key = f\"{patient_id}_{sepsis_label}\"\n",
    "    labeled_patients[new_key] = df\n",
    "\n",
    "# Optional: Print counts to verify split counts\n",
    "sepsis_count = sum(1 for key in labeled_patients if key.endswith('_1'))\n",
    "nonsepsis_count = sum(1 for key in labeled_patients if key.endswith('_0'))\n",
    "print(f\"Number of SEPSIS patients: {sepsis_count}\")\n",
    "print(f\"Number of NON-SEPSIS patients: {nonsepsis_count}\")\n",
    "\n",
    "# Now, create a list of keys and a matching list of binary labels for stratification.\n",
    "keys = list(labeled_patients.keys())\n",
    "labels = [1 if key.endswith('_sepsis') else 0 for key in keys]  # 1 = sepsis, 0 = no sepsis\n",
    "\n",
    "# Split the keys into train and test sets while maintaining the sepsis proportion.\n",
    "train_keys, test_keys, _, _ = train_test_split(\n",
    "    keys, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Build train and test dictionaries from the split keys.\n",
    "train_data_dict = {key: labeled_patients[key] for key in train_keys}\n",
    "test_data_dict = {key: labeled_patients[key] for key in test_keys}\n",
    "\n",
    "# Optional: Verify the stratification in your splits.\n",
    "train_sepsis = sum(1 for key in train_data_dict if key.endswith('_1'))\n",
    "test_sepsis = sum(1 for key in test_data_dict if key.endswith('_1'))\n",
    "print(f\"Train SEPSIS: {train_sepsis}, NON-SEPSIS: {len(train_data_dict) - train_sepsis}\")\n",
    "print(f\"Test SEPSIS: {test_sepsis}, NON-SEPSIS: {len(test_data_dict) - test_sepsis}\")\n",
    "\n",
    "# If needed, you can also concatenate these dictionaries back into DataFrames:\n",
    "train_df = pd.concat(train_data_dict.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c6ed77",
   "metadata": {},
   "source": [
    "#  Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e2755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aba5d05f",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122c2e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3668017d",
   "metadata": {},
   "source": [
    "# Maximise Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fac6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af516826",
   "metadata": {},
   "source": [
    "# Re-evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab00fc35-1093-4c36-ba17-571c40eef6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f023bb8a-095f-4275-b019-d73def0769b3",
   "metadata": {},
   "source": [
    "# Utility Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e0380-085d-4466-bd59-143b3441ce4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
