{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9b1a57-9275-4251-8942-24739d27660b",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12c36398-451d-47f2-80f6-b144335ca947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shayaan/anaconda3/envs/AppliedDataScience/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "# Import your custom modules. Adjust the module paths as needed.\n",
    "from src.data.load_data import loadTrainingData\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.features.create_feature_vectors import extract_features_with_expanding_window\n",
    "from src.plots.feature_plots import plot_roc_auc, plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report,fbeta_score,confusion_matrix\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"text\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828a24e3-604f-480b-b4dc-d4b64d27e42f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05489049-de4b-4388-a44c-b1fdd0ffa790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data from: ../../training_setA/*.psv with max_files=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PSV Files: 100%|██████████| 20336/20336 [00:35<00:00, 569.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data from: ../../training_setB/*.psv with max_files=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PSV Files: 100%|██████████| 20000/20000 [00:35<00:00, 563.04it/s]\n"
     ]
    }
   ],
   "source": [
    "directories = ['../../training_setA/', '../../training_setB/']\n",
    "max_files = None  # Adjust as needed\n",
    "\n",
    "patient_dict = {}\n",
    "\n",
    "for directory in directories:\n",
    "    pattern = os.path.join(directory, \"*.psv\")\n",
    "    print(f\"\\nLoading data from: {pattern} with max_files={max_files}\")\n",
    "    patient_data = loadTrainingData(\n",
    "        pattern,\n",
    "        max_files,\n",
    "        ignore_columns=['Unit2','Unit1','ICULOS','HospAdmTime']\n",
    "    )\n",
    "    patient_dict.update(patient_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db37c0e8-118e-47f3-9e01-b1fedb150dfd",
   "metadata": {},
   "source": [
    "# Create Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c028cab8-4b58-46ee-a4b9-3c3a7e81c9d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features with expanding window:   0%|          | 0/40336 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "Extracting features with expanding window:   0%|          | 16/40336 [00:00<21:45, 30.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n",
      "\n",
      "Intel MKL ERROR: Parameter 6 was incorrect on entry to DGELSD.\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "SVD did not converge in Linear Least Squares",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/shayaan/anaconda3/envs/AppliedDataScience/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/home/shayaan/anaconda3/envs/AppliedDataScience/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/shayaan/anaconda3/envs/AppliedDataScience/lib/python3.12/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/shayaan/Documents/sepsis/pipelines_le/mgbm_pipeline/src/features/create_feature_vectors.py\", line 82, in extract_features_for_patient_with_windows\n    features = extract_features_for_patient(partial_df)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/shayaan/Documents/sepsis/pipelines_le/mgbm_pipeline/src/features/create_feature_vectors.py\", line 63, in extract_features_for_patient\n    sw_feats = sliding_window_features(df_copy[col].values, window_sizes=[5])\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/shayaan/Documents/sepsis/pipelines_le/mgbm_pipeline/src/features/create_feature_vectors.py\", line 51, in sliding_window_features\n    slope, _ = np.polyfit(x, padded, 1)\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/shayaan/anaconda3/envs/AppliedDataScience/lib/python3.12/site-packages/numpy/lib/polynomial.py\", line 669, in polyfit\n    c, resids, rank, s = lstsq(lhs, rhs, rcond)\n                         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/shayaan/anaconda3/envs/AppliedDataScience/lib/python3.12/site-packages/numpy/linalg/linalg.py\", line 2326, in lstsq\n    x, resids, rank, s = gufunc(a, b, rcond, signature=signature, extobj=extobj)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/shayaan/anaconda3/envs/AppliedDataScience/lib/python3.12/site-packages/numpy/linalg/linalg.py\", line 124, in _raise_linalgerror_lstsq\n    raise LinAlgError(\"SVD did not converge in Linear Least Squares\")\nnumpy.linalg.LinAlgError: SVD did not converge in Linear Least Squares\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feature_df \u001b[38;5;241m=\u001b[39m extract_features_with_expanding_window(patient_dict)\n",
      "File \u001b[0;32m~/Documents/sepsis/pipelines_le/mgbm_pipeline/src/features/create_feature_vectors.py:98\u001b[0m, in \u001b[0;36mextract_features_with_expanding_window\u001b[0;34m(patient_dict)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features_with_expanding_window\u001b[39m(patient_dict: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m     96\u001b[0m   \u001b[38;5;66;03m# Directly create the feature DataFrame from the parallel job's output\u001b[39;00m\n\u001b[1;32m     97\u001b[0m   feature_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m---> 98\u001b[0m       Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)(\n\u001b[1;32m     99\u001b[0m           delayed(extract_features_for_patient_with_windows)(pid, df)\n\u001b[1;32m    100\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m pid, df \u001b[38;5;129;01min\u001b[39;00m tqdm(patient_dict\u001b[38;5;241m.\u001b[39mitems(), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting features with expanding window\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m       )\n\u001b[1;32m    102\u001b[0m   ))\n\u001b[1;32m    104\u001b[0m   \u001b[38;5;66;03m# all_features = []\u001b[39;00m\n\u001b[1;32m    105\u001b[0m   \u001b[38;5;66;03m# for pid, df in tqdm(patient_dict.items(), desc=\"Extracting features with expanding window\"):\u001b[39;00m\n\u001b[1;32m    106\u001b[0m   \u001b[38;5;66;03m#   patient_features = extract_features_for_patient_with_windows(pid, df)\u001b[39;00m\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;66;03m#   all_features.extend(patient_features)\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \n\u001b[1;32m    109\u001b[0m   \u001b[38;5;66;03m# feature_df = pd.DataFrame(all_features)\u001b[39;00m\n\u001b[1;32m    111\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal shape of expanded feature DataFrame:\u001b[39m\u001b[38;5;124m\"\u001b[39m, feature_df\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/AppliedDataScience/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/AppliedDataScience/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AppliedDataScience/lib/python3.12/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error_fast()\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AppliedDataScience/lib/python3.12/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     error_job\u001b[38;5;241m.\u001b[39mget_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/AppliedDataScience/lib/python3.12/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_or_raise()\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/AppliedDataScience/lib/python3.12/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mLinAlgError\u001b[0m: SVD did not converge in Linear Least Squares"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features with expanding window:   0%|          | 16/40336 [00:14<21:45, 30.89it/s]"
     ]
    }
   ],
   "source": [
    "feature_df = extract_features_with_expanding_window(patient_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808951b3",
   "metadata": {},
   "source": [
    " # Save or Load Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature_data(feature_df, file_path=\"feature_data.pkl\"):\n",
    "    feature_df.to_pickle(file_path)\n",
    "    print(f\"Feature data saved to {file_path}\")\n",
    "\n",
    "def load_feature_data(file_path=\"feature_data.pkl\"):\n",
    "    feature_df = pd.read_pickle(file_path)\n",
    "    print(f\"Feature data loaded from {file_path}\")\n",
    "    return feature_df\n",
    "\n",
    "# feature_df = load_feature_data()\n",
    "save_feature_data(feature_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75734e",
   "metadata": {},
   "source": [
    "# Add/Remove features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b7cca6-67b7-4ade-b12b-023841fb1f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63189df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e773c",
   "metadata": {},
   "source": [
    "# Split Sets into Test and Train on Patient ID of Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group rows by patient_id to create a patient-wise dictionary.\n",
    "patient_groups = {patient_id: group \n",
    "                  for patient_id, group in feature_df.groupby(\"patient_id\")}\n",
    "\n",
    "# Create a new dictionary with keys indicating sepsis status.\n",
    "# For each patient, if any row's SepsisLabel equals 1, mark that patient as sepsis.\n",
    "labeled_patients = {}\n",
    "for patient_id, df in patient_groups.items():\n",
    "    # Check if the patient ever had sepsis\n",
    "    sepsis_label = \"1\" if df[\"SepsisLabel\"].any() else \"0\"\n",
    "    new_key = f\"{patient_id}_{sepsis_label}\"\n",
    "    labeled_patients[new_key] = df\n",
    "\n",
    "# Optional: Print counts to verify split counts\n",
    "sepsis_count = sum(1 for key in labeled_patients if key.endswith('_1'))\n",
    "nonsepsis_count = sum(1 for key in labeled_patients if key.endswith('_0'))\n",
    "print(f\"Number of SEPSIS patients: {sepsis_count}\")\n",
    "print(f\"Number of NON-SEPSIS patients: {nonsepsis_count}\")\n",
    "\n",
    "# Now, create a list of keys and a matching list of binary labels for stratification.\n",
    "keys = list(labeled_patients.keys())\n",
    "labels = [1 if key.endswith('_sepsis') else 0 for key in keys]  # 1 = sepsis, 0 = no sepsis\n",
    "\n",
    "# Split the keys into train and test sets while maintaining the sepsis proportion.\n",
    "train_keys, test_keys, _, _ = train_test_split(\n",
    "    keys, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Build train and test dictionaries from the split keys.\n",
    "train_data_dict = {key: labeled_patients[key] for key in train_keys}\n",
    "test_data_dict = {key: labeled_patients[key] for key in test_keys}\n",
    "\n",
    "# Optional: Verify the stratification in your splits.\n",
    "train_sepsis = sum(1 for key in train_data_dict if key.endswith('_1'))\n",
    "test_sepsis = sum(1 for key in test_data_dict if key.endswith('_1'))\n",
    "print(f\"Train SEPSIS: {train_sepsis}, NON-SEPSIS: {len(train_data_dict) - train_sepsis}\")\n",
    "print(f\"Test SEPSIS: {test_sepsis}, NON-SEPSIS: {len(test_data_dict) - test_sepsis}\")\n",
    "\n",
    "# If needed, you can also concatenate these dictionaries back into DataFrames:\n",
    "train_df = pd.concat(train_data_dict.values(), ignore_index=True)\n",
    "test_df = pd.concat(test_data_dict.values(), ignore_index=True)\n",
    "\n",
    "train_df = train_df.drop(columns=['patient_id'])\n",
    "test_df = test_df.drop(columns=['patient_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c6ed77",
   "metadata": {},
   "source": [
    "#  Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X_train = train_df.drop(columns=[\"SepsisLabel\"], errors=\"ignore\")\n",
    "y_train = train_df[\"SepsisLabel\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"SepsisLabel\"], errors=\"ignore\")\n",
    "y_test = test_df[\"SepsisLabel\"]\n",
    "\n",
    "neg_samples, pos_samples = y_train.value_counts()\n",
    "neg_samples_test, pos_samples_test = y_test.value_counts()\n",
    "print(f\"Negative samples of Train: {neg_samples}, Positive samples of Train: {pos_samples}\")\n",
    "print(f\"Negative samples of Test: {neg_samples_test}, Positive samples of Test: {pos_samples_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe316a",
   "metadata": {},
   "source": [
    "# Optuna HyperParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import make_scorer\n",
    "# from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "# from optuna.terminator import report_cross_validation_scores\n",
    "# from optuna.visualization import plot_terminator_improvement\n",
    "# from plotly.io import show\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Define a search space for hyperparameters.\n",
    "#     params = {\n",
    "#         'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "#         'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "#         'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
    "#         'gamma': trial.suggest_loguniform('gamma', 1e-8, 1e1),\n",
    "#         'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "#         'objective': 'binary:logistic',\n",
    "#         'eval_metric': 'auc',\n",
    "#         'scale_pos_weight': neg_samples / pos_samples,\n",
    "#         'random_state': 42,\n",
    "#         'use_label_encoder': False\n",
    "#     }\n",
    "    \n",
    "#     # Use stratified 3-fold cross-validation.\n",
    "#     skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "#     clf = xgb.XGBClassifier(**params)\n",
    "    \n",
    "#     # Define a scoring function based on F_beta score with beta=4.5.\n",
    "#     scorer = make_scorer(fbeta_score, beta=4.5)\n",
    "#     scores = cross_val_score(clf, X_train, y_train, scoring=scorer, cv=skf)\n",
    "    \n",
    "#     # Return the mean F_beta score.\n",
    "#     return np.mean(scores)\n",
    "\n",
    "# # Create an Optuna study object to maximize the F_beta score.\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# print(\"Starting hyperparameter tuning with Optuna...\")\n",
    "# study.optimize(objective, n_trials=15)\n",
    "\n",
    "# fig = plot_terminator_improvement(study, plot_error=True)\n",
    "# show(fig)\n",
    "\n",
    "# # Output the best trial.\n",
    "# print(\"Best trial:\")\n",
    "# print(\"  F_beta Score: \", study.best_trial.value)\n",
    "# print(\"  Params: \", study.best_trial.params)\n",
    "\n",
    "# # Extract best parameters and add fixed parameters.\n",
    "# best_params = study.best_trial.params\n",
    "# best_params['objective'] = 'binary:logistic'\n",
    "# best_params['eval_metric'] = 'auc'\n",
    "# best_params['scale_pos_weight'] = neg_samples / pos_samples\n",
    "# best_params['random_state'] = 42\n",
    "# best_params['use_label_encoder'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e863f94-5775-43b3-bcf3-90267658a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Train model\n",
    "model = xgb.XGBClassifier(\n",
    "    random_state=42,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric=\"auc\",\n",
    "    scale_pos_weight=neg_samples / pos_samples\n",
    ")\n",
    "model.fit(X_train, y_train, \n",
    "          eval_set=[(X_test, y_test)],\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5d05f",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fbf838-1692-4fa1-877c-f89431cd21cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plots.feature_plots import plot_roc_auc, plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "plot_roc_auc(model, X_test, y_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, labels=(\"No Sepsis\", \"Sepsis\"))\n",
    "\n",
    "# Print classification metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac309b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3668017d",
   "metadata": {},
   "source": [
    "# Maximise Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7af989",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "\n",
    "f_beta_scores = []\n",
    "beta_val = 4.5\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_threshold = (y_proba >= t).astype(int)\n",
    "    fb = fbeta_score(y_test, y_pred_threshold, beta=beta_val)\n",
    "    f_beta_scores.append(fb)\n",
    "\n",
    "optimal_threshold = thresholds[np.argmax(f_beta_scores)]\n",
    "print(f\"Optimal threshold: {optimal_threshold}, F Beta {beta_val} Score: {max(f_beta_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af516826",
   "metadata": {},
   "source": [
    "# Re-evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for the positive class\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply the threshold to get the new predictions\n",
    "y_pred_custom = (y_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "\n",
    "plot_roc_auc(model, X_test, y_test, optimal_threshold)\n",
    "\n",
    "print(classification_report(y_test, y_pred_custom))\n",
    "print(confusion_matrix(y_test, y_pred_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fda3cd5",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ece945",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.feature_importances_\n",
    "features = X_test.columns\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "importance_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d547ba",
   "metadata": {},
   "source": [
    "# Shap Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88122c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f023bb8a-095f-4275-b019-d73def0769b3",
   "metadata": {},
   "source": [
    "# Utility Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc4f99-2cc5-450f-bd02-01d7b64df56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af620a6d-946a-4879-b58f-db03de617723",
   "metadata": {},
   "source": [
    "# Most Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8674a04-5892-4f27-ba18-9e91c4758cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AppliedDataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
